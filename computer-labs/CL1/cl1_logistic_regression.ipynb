{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general procedure for supervised learning\n",
    "\n",
    "**Note: This is part 2 of CL1, you should complete part 1: \"cl1_linear_regression.ipynb\" before continuing with this one.**\n",
    "\n",
    "In this second part you create a classification model.\n",
    "The goal is to show how these kind of problems is generally solved.\n",
    "It is also a good opportunity to get further acquainted with `pytorch`\n",
    "\n",
    "For most supervised learning tasks, the procedure we follow is comprised of the following steps:\n",
    "\n",
    "### Step 1: Data exploration\n",
    "The first step is normally to load the data and try to understand its properties. A few things that are usually useful:\n",
    "1. Check data formats.\n",
    "2. Visual inspection of data.\n",
    "3. Investigate (get some type of understanding for) how hard the problem is. \n",
    "\n",
    "\n",
    "### Step 2: Data preprocessing\n",
    "1. Normalise (or scale) input data. \n",
    "2. Convert the data to a different type, or organize it differently for the optimization (e.g. Numpy arrays, subsets of the dataset, etc.)\n",
    "3. Encode input and output data on a suitable form. For instance, we often use one-hot encoding to represent string variables.\n",
    "4. Split data into training, validation and test sets.\n",
    "\n",
    "\n",
    "### Step 3: Training\n",
    "1. Build a tentative network architecture (could be the simplest one you think could work, or based in previous sucesses).\n",
    "2. Select optimizer, performance measures and a few more hyperparameters. \n",
    "3. Train the network. \n",
    "4. Analyze performance on the training and validation sets. Adjust design decisions accordingly.\n",
    "\n",
    "\n",
    "### Step 4: Assessment\n",
    "1. Use the network for predictions in the test set.\n",
    "2. Evaluate the final quality of the model. **Note**: Once this is done, you shouldn't alter your model anymore, otherwise you need a new test set (if you want a good estimate of your model's generalisation capacity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply most of these steps to the task of correctly classifying an Iris plant, given its morphologic features present in the IRIS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we'll use all of the data, not only focusing on one of the species or a subset of the features. The `plot` method can help us obtain different types of visualizations of the data in the `DataFrame`. For instance, we can use it to plot histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='hist', bins=30, alpha=0.7, figsize=[15,6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat informative, but we could get an even better grasp of the data by first separating it into the different species (it seems likely that different species will have different feature distributions), and then plotting the histograms.\n",
    "\n",
    "However, if we let the `plot` method automatically create the histogram bins where it wants, each histogram might have different ranges, which would make it harder to compare them. Instead, we create the bins ourselves and pass that as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'species' column, so we get only the numeric values of the dataset\n",
    "features_dataset = dataset.drop('species', axis=1)\n",
    "\n",
    "# Find maximum and minimum values\n",
    "maxval = np.max(features_dataset.values)\n",
    "minval = np.min(features_dataset.values)\n",
    "\n",
    "# Create 30 linearly spaced numbers in this range\n",
    "my_bins = np.linspace(minval, maxval, 30)\n",
    "print(my_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the species\n",
    "species_names = dataset['species'].unique()\n",
    "print(species_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each species name, plot a histogram\n",
    "for name in species_names:\n",
    "    dataset[dataset[\"species\"]==name].plot(kind=\"hist\", bins=my_bins, alpha=0.7, figsize=[15,4], title=name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that different species do have substantial differences in the distributions of each feature, e.g. the Setosa species has shorter sepals than the others, etc. \n",
    "\n",
    "Another way to gain more insight about the data is using the method `pairplot`, from the seaborn python module. This shows scatter plots between all feature pairs (hence the time required to run it increases exponentially with the number of features!) and histograms for each feature, color-coded by the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also helpful to check if the dataset is balanced. We can do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in a dictionary with the number of ocurrences of each species\n",
    "n = {}\n",
    "for name in species_names:\n",
    "    extract_rule = dataset['species']==name\n",
    "    n[name] = len(dataset[extract_rule])\n",
    "    \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that each species occurs exactly 50 times in the dataset, so it's perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing input and output vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare the data for the training. The first thing we should do is define the input and the output arrays for our network. \n",
    "\n",
    "Defining the input is as simple as extracting only the numeric columns of the dataset (this can also be conveniently done using the `drop` method, as done before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical values\n",
    "# `.values` extracts the data as an numpy array\n",
    "x = dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "\n",
    "# Print first 10 rows\n",
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the output vector requires one more step, because of the way we'll train our network. Since the optimizer needs to be able to compare the predictions made by the neural network (i.e. a numeric vector), with the desired output vector in order to decide how to alter the weights, it's usually necessary to encode the output vector in a numeric format, instead of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['species'].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a handy function to map the species strings to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_species(species):\n",
    "    if species == 'setosa':\n",
    "        return 0\n",
    "    if species == 'versicolor':\n",
    "        return 1\n",
    "    if species == 'virginica':\n",
    "        return 2\n",
    "    else:\n",
    "        raise ValueError('Species \\'{}\\' is not recognized.'.format(species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And them we use `map` to apply it to every element in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = map(encode_species, dataset['species'].values)\n",
    "y = np.array(list(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, in order to assess how well our classifier generalizes to new, unseen data, we would like to withhold part of the dataset from the training process. This withheld part is usually called the test set. \n",
    "\n",
    "Scikit-learn conveniently provides `train_test_split` function for exactly this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method randomly chooses which examples will be withheld, and here we want the test set to be comprised of approximately 30% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `x_train` and `y_train` to train the network, and `x_test` and `y_test` to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create the `TensorDataset` and the `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make use of the `TensorDataset` and the `DataLoader` classes to make the training step simpler afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `numpy array` can always be made into a `torch.Tensor´\n",
    "torch_x = torch.tensor(x_train, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y_train, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = TensorDataset(torch_x, torch_y)\n",
    "t_data_loader = DataLoader(t_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we don't provide the argument `dtype`, the `tensor` function tries to infer the type of the `Tensor` that will be created from the type of the data supplied. In this case, this is a problem, since `x_train` is of type `float64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in deep learning usually memory is a bottleneck, so it's common to use `float32` for the parameters of the neural network. Having the input (in this case `x_train`) as `float64`, and the parameters of the net as `float32` causes `pytorch` to throw an error, and is a common mistake.\n",
    "\n",
    "We don't have the same problem with `y_train` because its type is already the correct one for the loss we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting up the optimization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a neural network which only has one hidden layer. The input to this hidden layer are the features for each sample, which are 4-dimensional. The output of it will be 3-dimensional, consisting of one number for each of the three classes, corresponding to how likely the model thinks the input is from that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model inherits from `nn.Module`, just as with the `LinearRegressor` in CL1.\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LogisticRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: to make sure you clearly understand what we're doing here, draw the network on a piece of paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we didn't use a softmax activation for this model. The loss we will use, `pytorch`'s [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), already applies the (log of the) softmax function for us.\n",
    "\n",
    "#### Clearing up `pytorch`'s NLL/Cross entropy confusion\n",
    "$\n",
    "\\def\\x{\\mathbf{x}}\n",
    "\\def\\yTrue{\\mathbf{y}}\n",
    "\\def\\z{\\mathbf{z}}\n",
    "\\def\\p{\\mathbf{p}}\n",
    "\\def\\p{\\mathbf{p}}\n",
    "$\n",
    "When implementing a classifier model in `pytorch` it is common to see two different loss functions:\n",
    "\n",
    "- [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
    "\n",
    "Both are valid options but they have slightly different behaviour so you need to know how to use them.\n",
    "\n",
    "First let's us formulate what we actually want our model to do.\n",
    "For classification problems we ultimately want a model that takes an input $\\x$ and predicts a probability vector $\\p$, where the element $\\p_c$ is the predicted probability of class $c$.\n",
    "This is typically done by transforming the actual network output $\\z$ with a softmax function:\n",
    "$$\n",
    "    \\p_c = \\frac{e^{\\z_c}}{\\sum_{c'=1}^{C} e^{\\z_{c'}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Our current network $f$ outputs the values $\\mathbf{z}$ that go *in to* the softmax, they are commonly called *logits*:\n",
    "$$\n",
    "\\mathbf{z} = f(\\mathbf{x}),\n",
    "$$\n",
    "note that the network outputs a tensor (a vector) of logits, one logit for each class.\n",
    "The `CrossEntropyLoss` $\\mathcal{L}_{CE}$ is happy with logits input, since it expects the ground truth $\\mathbf{y}$ as a one-hot vector (a vector with all zeroes, except at the true class where it is one), and the logits $\\mathbf{z}$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{z} = f(\\mathbf{x}), \\mathbf{y}) =\n",
    "- \\sum_{c=1}^{C} \\mathbf{y}_c \\log \\left(\n",
    "    \\frac{e^{\\mathbf{z}_c}}{\\sum_{c'=1}^{C} e^{\\mathbf{z}_{c'}}}\n",
    "\\right)\n",
    "=\n",
    "- \\sum_{c=1}^{C} \\mathbf{y}_c \\log \\left(\n",
    "    \\mathbf{p}_c\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "In the last equality we use the fact that the softmax produces a vector where the elements are between 0 and 1 and sum to 1. Therefore we can interpret the element $\\mathbf{p}_c$ as the predicted probability of class $c$.\n",
    "\n",
    "Suppose that the true class is $c^*$, then the only non-zero element of $\\mathbf{y}$ is $\\mathbf{y}_{c^*} = 1$,\n",
    "meaning that the loss above can be simplified:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{z} = f(\\mathbf{x}), \\mathbf{y}) =\n",
    "- \\log \\left(\n",
    "    \\mathbf{p}_c\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "If we instead modify our network into a new network $\\tilde{f}$, which ends with a softmax function,\n",
    "then the network itself would output a class probability vector $\\mathbf{p}$ and we would use the `NLLLoss`:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{NLL}(\\mathbf{p} = \\tilde{f}(\\mathbf{x}), \\mathbf{y}) =\n",
    "- \\log \\left(\n",
    "    \\mathbf{p}_c\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "As you can see, both versions calculate the same loss in the end but we had to carefully choose the correct combination of network architecture and loss.\n",
    "**Make sure your network outputs what your loss function expects!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# You can read the documentation with the `help` command.\n",
    "help(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performing the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can perform the optimization.\n",
    "Note how similar the optimization is to the linear regression task of CL1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    for b_x, b_y in t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    print('Epoch: {}\\tAvg loss: {}'.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the loss decreases as the optimization progresses. However, it's hard to know if a certain value of the loss, say 0.5, is good or bad, so we're not sure about the performance of the model. Because of this, it's usually informative to display other metrics of progress during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optimization + compute accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such metric that is easily interpretable is the accuracy of the model, defined as:\n",
    "\n",
    "$ Acc = \\frac{\\# \\text{Samples correctly classified}}{\\# \\text{Samples}} $\n",
    "\n",
    "From the definition, we see that accuracy is always a value between 0 and 1. An accuracy of 0 means that every single prediction made by our model is wrong, and an accuracy of 1 that our model is always correct.\n",
    "\n",
    "**Task**: what if our dataset was imbalanced? Would it still be a good idea to use accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if a certain sample was predicted correctly by the model, we'll use the highest predicted class as the choice to compare with ground truth. For instance, these are the input features and ground-truth for the first sample in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = t_dataset[0]\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the prediction according to our trained model.\n",
    "Remember that our network does not have a softmax function at the end.\n",
    "Instead the network outputs the logits. If we were to end with a softmax transformation,\n",
    "the class index with the highest probability would of course be the index with the largest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits predicted by the model\n",
    "model(sample_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the index of the highest prediction as the \"hard\" prediction of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sample_x).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the prediction with the ground truth, to know if the model was correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sample_x).argmax() == sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this for all samples during training for computing the accuracy. Additionally, the accuracy for each batch is not as informative as the accuracy in the entire dataset, so we will aggregate the number of correct predictions and display it once for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model and the optimizer\n",
    "model = LogisticRegressor()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Compute number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    accuracy = n_correct/len(t_dataset)\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    print('Epoch: {}\\tAvg loss: {:.3f} \\tAccuracy: {:.2f}'.format(epoch, avg_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's easier to interpret the model's performance (you should get a result close to 100% classification).\n",
    "\n",
    "**Task:** Were you expecting overfitting? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimization + validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't train the model for enough steps, it won't reach a satisfactory performance. If we train it for too many steps, it might overfit to the training data. To balance this tradeoff (and to tune hyper-parameters in general) training data is split into two sets.  One is used to actually perform the backpropagation and update the weights (usually to as the *training set*), and the other which is only used, not for backpropagation, but to assess the model's performance (usually referred to as the *validation set*). This way we can train the model with the training set and use the performance on the validation set to determine if we are overfitting.\n",
    "\n",
    "We can easily split our existing `t_dataset` using the `random_split` function from `pytorch`. This function accepts as first argument the dataset we want to split and as the second argument a sequence of lengths for the new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_t_dataset, val_t_dataset = random_split(t_dataset, [90, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create data loaders for each of the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_data_loader = DataLoader(train_t_dataset, batch_size=32, shuffle=True)\n",
    "val_t_data_loader = DataLoader(val_t_dataset, batch_size=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform the optimization, computing the train and validation loss and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model and the optimizer\n",
    "model = LogisticRegressor()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # Compute predictions and back-prop in the training set\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in train_t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_accuracy = n_correct/len(train_t_dataset)\n",
    "    train_avg_loss = sum(losses)/len(losses)    \n",
    "\n",
    "        \n",
    "    # Compute predictions in the validation set (with adagrad deactivated)\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_t_data_loader:\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "        val_accuracy = n_correct/len(val_t_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)      \n",
    "        \n",
    "        \n",
    "    display_str = 'Epoch {} '\n",
    "    display_str += '\\tLoss: {:.3f} '\n",
    "    display_str += '\\tLoss (val): {:.3f}'\n",
    "    display_str += '\\tAccuracy: {:.2f} '\n",
    "    display_str += '\\tAccuracy (val): {:.2f}'\n",
    "    print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see if the model is performing well and whether or not it's overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to be able to evaluate how well the model can predict the class of new, unseen samples. This was the reason for withholding part of our data from the training process, so that now we have fresh, unseen samples. \n",
    "\n",
    "The idea now is to use the trained model to predict the class of each new sample, given its features, and then compare the predicted label with the correct label for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "To compare the labels, we can use different techniques. As we saw before, we can compute the accuracy, but this time on the test set samples. However, although this helps us to evaluate the model's performance, it provides an incomplete picture. For instance, it doesn't explain the types of missclassifications we are doing.\n",
    "\n",
    "So that we can gather more information about the quality of our classifier, we'll also compute the confusion matrix of its predictions. The confusion matrix is a table layout of the predictions of the classifier, in which each row represents the labels of the predicted class and each column the labels of the correct class.\n",
    "\n",
    "---\n",
    "\n",
    "To illustrate, imagine we train a classifier on samples that are either from the 'dog' class or the 'cat' class. After training, we show it 50 new samples. 30 of these new samples are cats, and 20 are dogs.\n",
    "\n",
    "For the new cats, our classifier correctly predicts 28 of them, but in 2 samples it thinks they are from the 'dog' class. Further, the classifier correctly predicts 15 of the new dogs, and in 5 samples it thinks they are actually from the 'cat' class. \n",
    "\n",
    "The resulting confusion matrix for this example would be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "      <th colspan=\"2\"><b>Predicted label</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Cat</td>\n",
    "    <td>Dog</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td rowspan=\"2\"><b>True label</b></td>\n",
    "    <td>Cat</td>\n",
    "    <td>28</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dog</td>\n",
    "    <td>5</td>\n",
    "    <td>15</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the element $C_{ij}$ ($i$-th row, $j$-th column), corresponds to the number of predictions of class $i$, when the true known class was the $j$-th class. This is not universal: some sources define the confusion matrix as the transpose of the one shown here. However, `sklearn` defines confusion matrices like this, so we'll adhere to this definition.\n",
    "\n",
    "A handy way of computing the confusion matrix, given the predictions and the true labels, is to use the function [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) from the scikit-learn module.\n",
    "\n",
    "The first step is to compute our predictions in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(x_test, dtype=torch.float32)\n",
    "test_labels = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "preds = model(test_samples).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the accuracy, we'll do like we did during training, but for the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (preds == test_labels).sum().item()/len(preds)\n",
    "print(\"Accuracy: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we can compute the confusion matrix using the `confusion_matrix` method from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: What can you conclude from this confusion matrix? Which classes are easy/hard to separate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
