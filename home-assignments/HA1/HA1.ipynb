{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8092c3fd452a3245",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA1 - Cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0235e816fc98b0f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/05/20/10/03/cat-and-dog-775116_960_720.jpg\" alt=\"Image of cats and dogs\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4bb694612153106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this home assignment, we'll use the Kaggle dataset for the [Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). It is comprised of 25k colour images of dogs and cats. Our goal with this dataset will be to create a classifier that can tell us if the input image is of a cat or a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee9e2aee031325a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Using your cloud GPU\n",
    "As a way of helping you speed up the training process, each group gets access to a cloud instance with a GPU. Take a look at the [instructions folder](https://github.com/JulianoLagana/deep-machine-learning/blob/master/instructions/) to understand how to connect to an instance and use our tools there. You're free to use this limited resource as you see fit, but if you spend all your credits, you'll need a late day to obtain more (and you can only do this once).\n",
    "\n",
    "### Strong recommendation:\n",
    "In order to make the most out of your GPU hours, first try solving the initial part of this notebook (tasks 0-3) in your own computer (these tasks can be solved on the CPU), and leave most of the available hours for solving tasks 4-5, and refining your best model further (and, if you have the spare hours, experiment a bit!).\n",
    "\n",
    "### Working efficiently:\n",
    "Training for several epochs just to have your code break at the last validation step is incredibly frustrating and inefficient. Good practice is to first test long training runs with a much simpler dry-run: a single epoch, a few batches et c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7371c24b57c153e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Requirements:\n",
    "- Whenever we ask you to plot anything, be sure to add a title and label the axes. If you're plotting more than one curve in the same plot, also add a legend.\n",
    "- When we ask you to train an architecture, train it for a reasonable number of epochs. \"Reasonable\" here means you should be fairly confident that training for a higher number of epochs wouldn't impact your conclusions regarding the model's performance. When experimenting, a single epoch is often enough to tell whether your model setup has improved or not.\n",
    "\n",
    "\n",
    "Hints:\n",
    "- If you get errors saying you've exhausted the GPU resources, well, then you've exhausted the GPU resources. However, sometimes that's because `pytorch` didn't release a part of the GPU's memory. If you think your CNN should fit in your memory during training, try restarting the kernel and directly training only that architecture.\n",
    "- Every group has enough cloud credits to complete this assignment. However, this statement assumes you'll use your resources judiciously (e.g. always try the code first in your machine and make sure everything works properly before starting your instances) and **won't forget to stop your instance after using it,**  otherwise you might run out of credits.\n",
    "- Before starting, take a look at the images we'll be using. This is a hard task, don't get discouraged if your first models perform poorly (several participants in the original competition didn't achieve an accuracy higher than 60%).\n",
    "- Solving the [computer labs](https://github.com/JulianoLagana/deep-machine-learning/tree/master/computer-labs) is a good way to get prepared for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ee6d24346a80d85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 0. Imports\n",
    "\n",
    "In the following cell, add all the imports you'll use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-464a08ede00083a4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION ###\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, CenterCrop\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4821dc273028d702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Loading the data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ea049dea4713494",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first step is to head to the [Kaggle website for the cats and dogs competition](https://www.kaggle.com/c/dogs-vs-cats/data) and download the data from there. You should download both the test and train folders together in one zip file (there is a `Download all` button at the bottom of the page). Unfortunately, you need to create a Kaggle account for this.\n",
    "\n",
    "**Only necessary for tasks 4-6**: Downloading the data to your local computer is quite straight-forward. Sooner or later you will have to upload the data to the cloud instance and that is a bit more tricky. There are a few ways to do it:\n",
    "\n",
    " - Jupyter Notebook upload function. When starting the notebook server with the command `jupyter notebook` you are directed to a main page. In the top right corner there is an upload button.\n",
    " - Using [`scp`](https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/) to copy files via an ssh connection.\n",
    " - Using the [Kaggle CLI](https://github.com/Kaggle/kaggle-api). We have added it to the conda environment.\n",
    "\n",
    "For this assignment we will again need data loaders. Like before we need to create a `Dataset` to give as input to a `DataLoader`. \n",
    "Fortunately, this type of image data is quite common so we get some help from `pytorch`. We can use [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) to create a `Dataset` for our images. As long as our folder structure for the data conforms to the folder structure expected by `ImageFolder`, we can use it right out of the box and the `DataLoader` class will happily accept it as input.\n",
    "\n",
    "To use `ImageFolder` you should create a folder structure that resembles the following (obviously, the folder names are up to you):\n",
    "\n",
    "\n",
    "         small_train             small_val                train                   val\n",
    "              |                      |                      |                      |\n",
    "              |                      |                      |                      |\n",
    "        -------------          -------------          -------------          -------------\n",
    "        |           |          |           |          |           |          |           |\n",
    "        |           |          |           |          |           |          |           |\n",
    "      cats        dogs       cats        dogs       cats        dogs       cats        dogs\n",
    "\n",
    "\n",
    "The `small_train` and `small_val` folders have the training and validation samples for your smaller subset of the data, while the `train` and `val` folders contain all the samples you extracted from Kaggle's `train.zip`.\n",
    "This is just a convenient way of having a smaller dataset to play with for faster prototyping.\n",
    "\n",
    "We provide you a notebook that shows how to achieve this folder structure (`create_project_notebook_structure.ipynb`), starting from the original `dogs-vs-cats.zip` file that you download from Kaggle. If you do use that notebook, we encourage you to understand how each step is being done, so you can generalize this knowledge to new datasets you'll encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89ba19509b952af2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For the smaller dataset, we advise you to use 70% of the data as training data (and thereby the remaining 30% for validation data). However, for the larger dataset, you should decide how to split between training and validation.\n",
    "\n",
    "**What percentage of the larger dataset did you decide to use for training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7f3b0dfbd90a14c1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8964386e29e42eee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optional (1 POE):** Did you decide to keep the same ratio split between train and validation sets for the larger dataset? Motivate your decision!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-88d41e16176067ca",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-876ca7df88c9311f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fill in the dataset paths (to be used later by your data loaders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1b1314f2ab1b1d6b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Change the directories accordingly\n",
    "train_path = \"/your/path\"\n",
    "val_path = \"/your/path\"\n",
    "small_train_path = \"/your/path\"\n",
    "small_val_path = \"/your/path\"\n",
    "### BEGIN SOLUTION\n",
    "train_path = \"train\"\n",
    "val_path = \"val\"\n",
    "small_train_path = \"small_train\"\n",
    "small_val_path = \"small_val\"\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d6ea64bca94a4ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### 1.1 Preprocessing\n",
    "**(1 POE)** \n",
    "\n",
    "Once you have the expected folder structure, create two data loaders for automatically generating batches from the images in your smaller subset of data. It is here we choose how to preprocess the input data. There are multiple reasons for why we preprocess data:\n",
    "\n",
    "- Some transformations might be needed to actually make the data work with our network (reshaping, permuting dimensions et c.).\n",
    "- Make the training more efficient by making the input dimensions smaller, e.g. resizing, cropping.\n",
    "- Artificially expanding the training data through [data augmentation](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n",
    "- We have some clever idea of how to change the data to make the training process better.\n",
    "\n",
    "We do not expect you to do data augmentation, but feel free to preprocess the data as you see fit.\n",
    "Construct an `ImageFolder` dataset like this:\n",
    "\n",
    "```python\n",
    "ImageFolder(<path_to_data_folder>, transform=Compose(<list_of_transforms>))\n",
    "# example:\n",
    "ImageFolder(Path.cwd() / \"small_train\", transform=Compose([ToTensor]))\n",
    "```\n",
    "\n",
    "Hints:\n",
    "- Take a look at [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) and [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) from the pytorch docs.\n",
    "- To preprocess the data you can use the built-in pytorch [`Transforms`](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "- The `ImageFolder` dataset provides the data as a python image type. For easy conversion to a `torch.Tensor`, use the [`ToTensor`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) transformation.\n",
    "- The specified `batch_size` should be chosen so that you train fast but don't run out of memory. You need to figure this out empirically; start small and increase the batch size until you run out of memory.\n",
    "- The `DataLoader` constructor takes an optional argument `num_workers`, which defaults to `0` if not provided. Setting a higher number creates multiple threads which load batches concurrently. This can speed up training considerably.  \n",
    "- When feeding the images to your CNN, you'll probably want all of them to have the same spatial size, even though the .jpeg files differ in this. Resizing the images can be done using the previously mentioned built-in pytorch Transforms.\n",
    "- Resizing the images to a smaller size while loading them can be beneficial. The VGG network that is used later in this assignment requires that images are at least 224x224, but before that use small images to speed up training. The CNN's do surprisingly well on 64x64 or even 32x32 images. Shorter training cycles give your more time to experiment!\n",
    "\n",
    "We encourage you to explore the data and choose transformations that you believe to be useful. For exploration we provide you with some helper functions to visually compare transformations side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ca8fc808d4ee65b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of ImageFolder datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "    \n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "    \n",
    "    Args:\n",
    "        transformations (list(ImageFolder)): list of ImageFolder instances with different transformations\n",
    "        index (int): Index of the sample in the ImageFolder you wish to compare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we combine two neat functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, like a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, ImageFolder) for transf in transformations):\n",
    "        raise TypeError(\"All elements in the `transformations` list need to be of type ImageFolder\")\n",
    "        \n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "    \n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(\"transf: {}\".format(counter))\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as image\n",
    "    \n",
    "    Example usage:\n",
    "        _, axis = plt.subplots()\n",
    "        some_random_index = 453\n",
    "        image_tensor, _ = train_dataset[some_random_index]\n",
    "        display_image(axis, image_tensor)\n",
    "    \n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\"The `display_image` function expects a `torch.Tensor` \" +\n",
    "                        \"use the `ToTensor` transformation to convert the images to tensors.\")\n",
    "        \n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31b81f052b6e681e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# 1 POE for resize, 1 POE for other preprocessing\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 64\n",
    "\n",
    "transformations = [Resize(size=[img_width,img_width]),ToTensor()]\n",
    "train_dataset = ImageFolder(small_train_path,transform=Compose(transformations))\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder(small_val_path,transform=Compose(transformations))\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "pil_type = ImageFolder(small_train_path,transform=Compose([CenterCrop(size=(img_width,img_width))]))\n",
    "\n",
    "\n",
    "# Sub-optimal transformation for comparison\n",
    "crop = ImageFolder(small_train_path,transform=Compose([CenterCrop(size=(img_width,img_width)),ToTensor()]))\n",
    "\n",
    "compare_transforms([train_dataset, crop], 0)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-24463974ae20a276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 POE)** How did you select transformations, if any? Briefly explain your reasoning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-051ee24a83af3cf8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0bfc1ac7fadfcc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Training\n",
    "\n",
    "**(1 POE)**\n",
    "\n",
    "Create your first CNN architecture for this task. Start with something as simple as possible, that you're almost sure can get an accuracy better than 50% (we'll improve upon it later).\n",
    "Naturally, you must also select a loss function and an optimizer.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Training on a CPU is slow and in the beginning you just want to verify that your architecture actually produces a predicition with the correct shape. Make everything you can to speed up the prototyping phase, e.g. train only for a single epoch and make the images ridiculously small.\n",
    "- Going from the last CNN layer to the final fully connected layer is not trivial. The convolutions produces \"3D\" output which we can think of as an image with many channels, while the fully connected layer expects a row vector as input. Calculate how many output features the convolutions produce and use `.reshape` to make your tensor fit the fully connected layer. (It is also common to see the `.view` method to do the same thing. They basically do the same thing but have some differences in internal memory management.) *Hint within the hint:* remember that the fully connected layers expects a *batch* of 1D tensors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c9de348cd8bc4ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Dog/Cat classifier\n",
    "    \n",
    "    Expects square input images with side = `size: int`\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        super(Net, self).__init__()\n",
    "        final_size = self._size_reduction(size)\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        # 224 inputs -(conv)-> 222 -(pool)-> 111 -(conv)-> 109 -(pool)-> 54\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(10 * final_size * final_size, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.out = nn.Sigmoid()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _size_reduction(x):\n",
    "        \"\"\"Heuristic size reduction\n",
    "        \n",
    "        Convolutions remove a border of size 2,\n",
    "        Max pooling halves the size.\n",
    "        ==> round( { [(x-2) / 2] - 2} / 2} = round(x/4 - 3/2)\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(x/4 - 3/2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape((batch_size, self.num_flat_features(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # The data loader return ground truth batch y with shape (batch_size,)\n",
    "        # squeeze turns our prediction from shape (batch_size, 1) -> (batch_size,)\n",
    "        x = self.out(x).squeeze()\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "model = Net(size=img_width)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb6fc78116ad6b75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train your model using the two data loaders you created earlier. Train for a reasonable amount of epochs, so as to get a good sense of how well this architecture performs.\n",
    "\n",
    "Hints:\n",
    "- Note that you will need to plot your training and validation losses and accuracies, so make sure that you saved them during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bb1fcd878f3bea9a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Any pytorch object (e.g. model, inputs, output, etc.) can \n",
    "# be transferred to the current device by running\n",
    "#       name_of_object.to(device)\n",
    "# Example:\n",
    "#       model.to(device)\n",
    "#\n",
    "# The following line automatically figures out what device (cpu or gpu)\n",
    "# you are using and stores the result in `device`.\n",
    "# Later we can use the `.to(device)` method to move our data or model to the correct device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION  \n",
    "from time import time\n",
    "model.to(device)\n",
    "num_epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_every = 50\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "start = time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (x, y) in enumerate(train_dataloader):\n",
    "        steps += 1\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = criterion(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        z[z >= 0.5] = 1\n",
    "        z[z < 0.5] = 0\n",
    "\n",
    "        equals = z == labels.reshape(z.shape).float()\n",
    "        running_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (x, y) in enumerate(val_dataloader):\n",
    "                    inputs, labels = x.to(device), y.to(device)\n",
    "                    z = model.forward(inputs)\n",
    "                    batch_loss = criterion(z, labels.float())\n",
    "                    val_loss += batch_loss.item()\n",
    "                    z[z >= 0.5] = 1\n",
    "                    z[z < 0.5] = 0\n",
    "                    \n",
    "                    equals = z == labels.reshape(z.shape).float()\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/print_every)\n",
    "            val_losses.append(val_loss/len(val_dataloader))    \n",
    "            train_accs.append(running_acc/print_every) \n",
    "            val_accs.append(accuracy/len(val_dataloader)) \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Train accuracy: {running_acc/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {val_loss/len(val_dataloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(val_dataloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            model.train()\n",
    "\n",
    "end = time()\n",
    "epoch_time = (end - start) / num_epochs\n",
    "print(\"| {} | {:.2f} | {:.3f} |\".format(img_width, epoch_time, val_accs[-1]))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d42c86687697a67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fa81712e1e27432a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, train_losses, 'r', xvalues, val_losses, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, train_accs, 'r', xvalues, val_accs, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2fc166890962bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Based on these, what would you suggest for improving your model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-506e21ce469b67f5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee79a83a62b70a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5314d286e79e0377",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** Continue to improve your model architecture by comparing the value of the metrics you're interested in for both the training and validation set. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here. Save the training and validation losses and accuracies. You'll use this later to compare your best model with the one using transfer learning.\n",
    "\n",
    "**Note**: When trying different ideas, you'll end up with several different models. However, when submitting your solutions to Canvas, the cell below must contain only the definition and training of *one* model. Remove all code related to the models that were not chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6edb7d7e343ab14b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Net, self).__init__()\n",
    "        final_size = self._size_reduction(size)\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 10, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(10 * final_size * final_size, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _size_reduction(x):\n",
    "        \"\"\"Heuristic size reduction\n",
    "        \n",
    "        Convolutions remove a border of size 2,\n",
    "        Max pooling halves the size.\n",
    "        ==> round( { [(x-2) / 2] - 2} / 2} = round(x/4 - 3/2)\n",
    "        \"\"\"\n",
    "        \n",
    "        return int(x/4 - 3/2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # Batch size can vary (train vs validation or rest batch at epoch end),\n",
    "        # set dynamically.\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape((batch_size, self.num_flat_features(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x).squeeze()\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "model = Net(img_width)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model.to(device)    \n",
    "\n",
    "num_epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_every = 50\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "start = time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (x, y) in enumerate(train_dataloader):\n",
    "        steps += 1\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = criterion(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        z[z >= 0.5] = 1\n",
    "        z[z < 0.5] = 0\n",
    "\n",
    "        equals = z == labels.reshape(z.shape).float()\n",
    "        running_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (x, y) in enumerate(val_dataloader):\n",
    "                    inputs, labels = x.to(device), y.to(device)\n",
    "                    z = model.forward(inputs)\n",
    "                    batch_loss = criterion(z, labels.float())\n",
    "                    val_loss += batch_loss.item()\n",
    "                    z[z >= 0.5] = 1\n",
    "                    z[z < 0.5] = 0\n",
    "                    \n",
    "                    equals = z == labels.reshape(z.shape).float()\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/print_every)\n",
    "            val_losses.append(val_loss/len(val_dataloader))    \n",
    "            train_accs.append(running_acc/print_every) \n",
    "            val_accs.append(accuracy/len(val_dataloader)) \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Train accuracy: {running_acc/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {val_loss/len(val_dataloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(val_dataloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "\n",
    "end = time()\n",
    "epoch_time = (end - start) / num_epochs\n",
    "print(epoch_time)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d033937b5a8b9875",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3df999674672de47",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, train_losses, 'r', xvalues, val_losses, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, train_accs, 'r', xvalues, val_accs, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a827c39d9e652e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 POE)** Did your results improve? What problems did your improvements fix? Explain why, or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cbda4b585ad39ddc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c67bcc4fbec1808e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Save your model](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to disk (the architecture, weights and optimizer state). This is simply so you can use it again easily in the later parts of the notebook, without having to keep it in memory or re-training it. The actual file you create is not relevant to your submission. The code to save the model is given in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that you called your model \"my_model\"\n",
    "torch.save(model.state_dict(), \"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25f9cc8d17491d0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cf9b347fc3ee9255",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**From now, training on CPU will not be feasible. If your computer has a GPU, try it out! Otherwise, now is the time to connect to your cloud instance**\n",
    "\n",
    "Now, instead of trying to come up with a good architecture for this task, we'll use the VGG16 architecture, but with the top layers removed (the fully connected layers + softmax). We'll substitute them with a single fully connected layer, and a classification layer that makes sense for our problem.\n",
    "\n",
    "However, this model has a very high capacity, and will probably suffer a lot from overfitting if we try to train it from scratch, using only our small subset of data. Instead, we'll start the optimization with the weights obtained after training VGG16 on the ImageNet dataset.\n",
    "\n",
    "Start by loading the *pretrained* VGG16 model, from the [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-01ebc4c9c306b985",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "VGG_model = models.vgg16(pretrained=True)\n",
    "print(VGG_model.classifier[0].in_features) # 1000 \n",
    "\n",
    "#VGG_model = applications.VGG16(weights=\"imagenet\", include_top=False, input_shape=(115,115,3))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-faed8047ef25a60d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create a new model with the layers you want to add on top of VGG.\n",
    "\n",
    "*Hint:*\n",
    "- You can access and modify the top layers of the VGG model with `vgg_model.classifier`, and the remaining layers with `vgg_model.features`.\n",
    "- You can get the number of output features of `vgg_model.features` with `vgg_model.classifier[0].in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56cb37360051a638",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "top_layers = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=256, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(in_features=256, out_features=1, bias=True),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d746f9eb61e3ea44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now add the new model on top of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-76e4aad7fbcf5d05",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "VGG_model.classifier = top_layers\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f76d1a7f6280af0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.1 Using VGG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-270f8ec140ddfba3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we're almost ready to train the new model. However, since the top layers of this architecture are being initialized randomly, it's sometimes possible for them to generate large gradients that can wreck the pretraining of the bottom layers. To avoid this, freeze all the VGG layers in your architecture (i.e. signal to the optimizer that these should not be changed during optimization) by setting the attribute `requires_grad` for all parameters `vgg_model.features` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bfb58ea46c31df0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Freeze bottom\n",
    "### BEGIN SOLUTION\n",
    "for param in VGG_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b508ede3d760a86b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Perform the transfer learning by training the top layers of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a025e60545ca151",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from time import time\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "VGG_model.to(device)\n",
    "optimizer = optim.Adam(VGG_model.parameters())\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_every = 50\n",
    "train_losses_vgg, val_losses_vgg, train_accs_vgg, val_accs_vgg = [], [], [], []\n",
    "\n",
    "start = time()\n",
    "\n",
    "VGG_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (x, y) in enumerate(train_dataloader):\n",
    "        steps += 1\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = VGG_model.forward(inputs).squeeze()\n",
    "        loss = criterion(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        z[z >= 0.5] = 1\n",
    "        z[z < 0.5] = 0\n",
    "\n",
    "        equals = z == labels.reshape(z.shape).float()\n",
    "        running_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            VGG_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (x, y) in enumerate(val_dataloader):\n",
    "                    inputs, labels = x.to(device), y.to(device)\n",
    "                    z = VGG_model.forward(inputs).squeeze()\n",
    "                    batch_loss = criterion(z, labels.float())\n",
    "                    val_loss += batch_loss.item()\n",
    "                    z[z >= 0.5] = 1\n",
    "                    z[z < 0.5] = 0\n",
    "                    \n",
    "                    equals = z == labels.reshape(z.shape).float()\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses_vgg.append(running_loss/print_every)\n",
    "            val_losses_vgg.append(val_loss/len(val_dataloader))    \n",
    "            train_accs_vgg.append(running_acc/print_every) \n",
    "            val_accs_vgg.append(accuracy/len(val_dataloader)) \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Train accuracy: {running_acc/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {val_loss/len(val_dataloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(val_dataloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            VGG_model.train()\n",
    "end = time()\n",
    "vgg_epoch_time = (end - start) / num_epochs\n",
    "print(\"Average epoch time: {}\".format(vgg_epoch_time))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad79e1aa5c4a6185",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f17c882b2a09dee7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses_vgg))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, train_losses_vgg, 'r', xvalues, val_losses_vgg, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, train_accs_vgg, 'r', xvalues, val_accs_vgg, 'b')\n",
    "plt.legend([\"Train\",\"Val\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-779d477ffe1ebbf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model obtained in step 3? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e3e3990ba39bea67",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses_vgg))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, val_losses, 'r', xvalues, val_losses_vgg, 'b')\n",
    "plt.legend([\"First Model\",\"VGG\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, val_accs, 'r', xvalues, val_accs_vgg, 'b')\n",
    "plt.legend([\"First Model\",\"VGG\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b84dd461d5ddcc8d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Compare these results. Which approach worked best, starting from scratch or doing transfer learning? Reflect on whether your comparison is fair or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f9e1a6a643946cd2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8afb448c67da5f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** What are the main differences between the ImageNet dataset and the Dogs vs Cats dataset we used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2be321b63232ae01",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71a8b8de004f6e57",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Optional (2 POE)** Even though there are considerable differences between these datasets, why is it that transfer learning is still a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-655d00face15a862",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19785940b9624d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Optional (1 POE)** In which scenario would transfer learning be unsuitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e79df7472ff5506a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-111f2b1d28919293",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VGG_model.state_dict(), \"trans_learning_top_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-544a73726bebe121",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ee9ebc87fd3358e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have a better starting point for the top layers, we can train the entire network. Unfreeze the bottom layers by resetting the `requires_grad` attribute to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3918c2cdd9817f7e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# UnFreeze bottom\n",
    "### BEGIN SOLUTION\n",
    "for param in VGG_model.features.parameters():\n",
    "    param.requires_grad = True\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80fa8c89f1b262f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fine tune the model by training all the layers.\n",
    "\n",
    "Hint:\n",
    "- Even though we do have a decent starting point for the optimization, it's still possible that a bad hyper-parameter choice wrecks the preinitialization. Make sure to use a small learning rate for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-594c6039216461e5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "VGG_model.to(device)\n",
    "optimizer = optim.Adam(VGG_model.parameters(),lr=0.0001)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_every = 50\n",
    "train_losses_vgg_f, val_losses_vgg_f, train_accs_vgg_f, val_accs_vgg_f = [], [], [], []\n",
    "\n",
    "start = time()\n",
    "VGG_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, (x, y) in enumerate(train_dataloader):\n",
    "        steps += 1\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = VGG_model.forward(inputs).squeeze()\n",
    "        loss = criterion(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        z[z >= 0.5] = 1\n",
    "        z[z < 0.5] = 0\n",
    "\n",
    "        equals = z == labels.reshape(z.shape).float()\n",
    "        running_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            VGG_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (x, y) in enumerate(val_dataloader):\n",
    "                    inputs, labels = x.to(device), y.to(device)\n",
    "                    z = VGG_model.forward(inputs).squeeze()\n",
    "                    batch_loss = criterion(z, labels.float())\n",
    "                    val_loss += batch_loss.item()\n",
    "                    z[z >= 0.5] = 1\n",
    "                    z[z < 0.5] = 0\n",
    "                    \n",
    "                    equals = z == labels.reshape(z.shape).float()\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses_vgg_f.append(running_loss/print_every)\n",
    "            val_losses_vgg_f.append(val_loss/len(val_dataloader))    \n",
    "            train_accs_vgg_f.append(running_acc/print_every) \n",
    "            val_accs_vgg_f.append(accuracy/len(val_dataloader)) \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Train accuracy: {running_acc/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {val_loss/len(val_dataloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(val_dataloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            VGG_model.train()\n",
    "end = time()\n",
    "vgg_fine_tune_epoch = (end - start) / num_epochs\n",
    "print(vgg_fine_tune_epoch)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5dc3e388a41da3ed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model trained with frozen layers? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7edb12ee397ec817",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses_vgg))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, val_losses_vgg, 'r', xvalues, val_losses_vgg_f, 'b')\n",
    "plt.legend([\"VGG (frozen)\",\"VGG (unfrozen)\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, val_accs_vgg, 'r', xvalues, val_accs_vgg_f, 'b')\n",
    "plt.legend([\"VGG (frozen)\",\"VGG (unfrozen)\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5dae528a81d5ff24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** Did the model's performance improve? Why (why not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0f4a5edca490320e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4ed3967e4f6c5f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trans_learning_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56908ee1e60aa411",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.3 Improving the top model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c8d8e5ab949ee35",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Improve the architecture for the layers you add on top of VGG16. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22d09c8401d84b61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-48933baad6c5afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** How does the model perform, compared to the model trained in step 4.2? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7cb62a04916a848e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bbfa3e11e2dfff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_trans_learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad0efbac33de5a65",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5. Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cf811afdac96843b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll train the model that achieved the best performance so far using the entire dataset.\n",
    "\n",
    "**Note**: start the optimization with the weights you obtained training in the smaller subset, i.e. *not* from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ae2a65188e4ac74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, create two new data loaders, one for training samples and one for validation samples. This time, they'll load data from the folders for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-64eaa83780f5eac9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = ImageFolder(train_path, transform=Compose([Resize(size=[img_width,img_width]), ToTensor()]))\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder(val_path, transform=Compose([Resize(size=[img_width,img_width]), ToTensor()]))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batches_per_epoch = len(train_dataset) // batch_size + 1\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3f79586de42561b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train your model using the full data. This optimization might take a long time, so live plotting of some metrics is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7dd71a632b5f152",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "VGG_model.to(device)\n",
    "optimizer = optim.Adam(VGG_model.parameters(),lr=0.0001)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "running_loss = 0\n",
    "running_acc = 0\n",
    "print_every = 1000\n",
    "train_losses_vgg_all, val_losses_vgg_all, train_accs_vgg_all, val_accs_vgg_all = [], [], [], []\n",
    "\n",
    "start = time()\n",
    "VGG_model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Train:\n",
    "    running_metric_ticker = 0\n",
    "    train_time = time()\n",
    "    for batch_index, (x, y) in enumerate(train_dataloader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = VGG_model.forward(inputs).squeeze()\n",
    "        loss = criterion(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        z[z >= 0.5] = 1\n",
    "        z[z < 0.5] = 0\n",
    "\n",
    "        equals = z == labels.reshape(z.shape).float()\n",
    "        running_acc += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        running_metric_ticker += 1\n",
    "        if batch_index % print_every == 0 or batch_index == batches_per_epoch:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            VGG_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x, y in val_dataloader:\n",
    "                    inputs, labels = x.to(device), y.to(device)\n",
    "                    z = VGG_model.forward(inputs).squeeze()\n",
    "                    batch_loss = criterion(z, labels.float())\n",
    "                    val_loss += batch_loss.item()\n",
    "                    z[z >= 0.5] = 1\n",
    "                    z[z < 0.5] = 0\n",
    "                    \n",
    "                    equals = z == labels.reshape(z.shape).float()\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses_vgg_all.append(running_loss/print_every)\n",
    "            val_losses_vgg_all.append(val_loss/len(val_dataloader))    \n",
    "            train_accs_vgg_all.append(running_acc/print_every) \n",
    "            val_accs_vgg_all.append(accuracy/len(val_dataloader)) \n",
    "            print(\"Epoch {}/{}, batch {}/{}\"\n",
    "                  .format(epoch, num_epochs, batch_index, batches_per_epoch))\n",
    "            print(\"Train loss: {:.3f}\\t Train acc: {:.3f}\\tVal loss: {:.3f}\\t Val acc: {:.3f}\"\n",
    "                  .format(running_loss / running_metric_ticker,\n",
    "                          running_acc / running_metric_ticker,\n",
    "                          val_loss / len(val_dataloader),\n",
    "                          accuracy / len(val_dataloader)))\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            VGG_model.train()\n",
    "            running_metric_ticker = 0\n",
    "end = time()\n",
    "full_train_epoch = (end - start) / num_epochs\n",
    "print(full_train_epoch)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1861d3a543c6386",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform now when trained on the entire dataset, compared to when only trained on the smaller subset of data? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ceaac6be60ce36a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "xvalues = range(len(train_losses_vgg_f))\n",
    "plt.figure(1)\n",
    "plt.subplot(311)\n",
    "plt.plot(xvalues, val_losses_vgg_f, 'r', xvalues, val_losses_vgg_all, 'b')\n",
    "plt.legend([\"VGG (unfrozen)\",\"VGG (all)\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(xvalues, val_accs_vgg_f, 'r', xvalues, val_accs_vgg_all, 'b')\n",
    "plt.legend([\"VGG (unfrozen)\",\"VGG (all)\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b38092b08c150e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** What can you conclude from these plots? Did you expect what you observe in the plots, explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-694a3fbb7f081da8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e1ddfbfceb4d194",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 6. Evaluation on test set (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a97630bf5d85363f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll evaluate your final model, obtained in step 6, on the test set. As mentioned before, the samples in the test set are not labeled, so we can't compute any performance metrics ourselves. \n",
    "\n",
    "As a bit of fun and to inspire some friendly competition you may instead submit it to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a8fded54ed7011",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Compute the predictions for all samples in the test set according to your best model, and save it in a .csv file with the format expected by the competition.\n",
    "\n",
    "Hints:\n",
    "- There is a `sampleSubmission.csv` file included in the zip data. Take a look at it to better understand what is the expected format here.\n",
    "- `pathlib`'s `Path` class has a `glob` function, which returns the filenames of all files in a given path.\n",
    "- If you don't know how to create and write to files with Python, Google can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cc77ac7849f856e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "size = (224, 224)\n",
    "\n",
    "submission_file = Path.cwd() / \"submission.csv\"\n",
    "if submission_file.exists():\n",
    "    print(\"Removing old submission file.\")\n",
    "    submission_file.unlink()\n",
    "    \n",
    "test_path = Path.cwd() / \"test\"\n",
    "# Need to list the images to get the number of matching files\n",
    "test_images = list(test_path.glob(\"*.jpg\"))\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "predictions = np.empty((num_test_images, ))\n",
    "with open(submission_file,\"a\") as f:\n",
    "    for counter, name in enumerate(test_images, 1):\n",
    "        img = Image.open(name)\n",
    "        img = img.resize(size, Image.BILINEAR)\n",
    "        transf = transforms.ToTensor()\n",
    "        inputs = transf(img)\n",
    "        inputs = inputs.unsqueeze(0).to(device)\n",
    "        id_ = int(name.stem)\n",
    "        predictions[id_ - 1] = VGG_model.forward(inputs).item()\n",
    "        if counter % 1000 == 0 or counter == num_test_images:\n",
    "            print(\"Processed {}/{} images\".format(counter, num_test_images))\n",
    "            \n",
    "with open(submission_file,\"w\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for id_, pred in enumerate(predictions, 1):\n",
    "        f.write(\"{},{}\\n\".format(id_, pred))\n",
    "        \n",
    "print(\"Predictions written to {}\".format(submission_file.name))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-faf8664f26ff7f4e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you created your submission file, submit it to Kaggle for evaluation. The [old competition](https://www.kaggle.com/c/dogs-vs-cats) does not allow submissions any more, but you can submit your file to the [new one](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) via the \"Late submission\" button (they use the same data). The Kaggle CLI can be used as well. Kaggle evaluates your submission according to your log-loss score. Which score did you obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e951dcec64dec85d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc362abcfef32eae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What was the username you used for this submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d519532bb1f957c3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer**: (fill in here)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
